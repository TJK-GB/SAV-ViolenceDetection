{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1AkskOvfum83j9TA8n3BbpEX6uEw-9P30","authorship_tag":"ABX9TyNFRuHN2CzLz2LXMg78RGY2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24ooGEVJNTVk","outputId":"1f63ed50-9fbe-45ea-baed-58d952b3ea5a","executionInfo":{"status":"ok","timestamp":1755173196100,"user_tz":-60,"elapsed":9193274,"user":{"displayName":"Taejin Kim","userId":"03444209368555110847"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/20: 100%|██████████| 335/335 [15:07<00:00,  2.71s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 0.7087 | Macro F1: 0.6364 | Micro F1: 0.6368\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/20: 100%|██████████| 335/335 [07:07<00:00,  1.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 0.5724 | Macro F1: 0.7284 | Micro F1: 0.7294\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/20: 100%|██████████| 335/335 [07:03<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 0.4961 | Macro F1: 0.7660 | Micro F1: 0.7683\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/20: 100%|██████████| 335/335 [06:56<00:00,  1.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 0.3897 | Macro F1: 0.8269 | Micro F1: 0.8281\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/20: 100%|██████████| 335/335 [06:52<00:00,  1.23s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 0.3375 | Macro F1: 0.8571 | Micro F1: 0.8580\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/20: 100%|██████████| 335/335 [06:48<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 | Loss: 0.2838 | Macro F1: 0.8852 | Micro F1: 0.8864\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/20: 100%|██████████| 335/335 [06:45<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 | Loss: 0.2572 | Macro F1: 0.9046 | Micro F1: 0.9058\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/20: 100%|██████████| 335/335 [06:49<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 | Loss: 0.2256 | Macro F1: 0.9140 | Micro F1: 0.9148\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/20: 100%|██████████| 335/335 [06:46<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 | Loss: 0.1781 | Macro F1: 0.9197 | Micro F1: 0.9208\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/20: 100%|██████████| 335/335 [06:46<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Loss: 0.2275 | Macro F1: 0.9155 | Micro F1: 0.9163\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/20: 100%|██████████| 335/335 [06:41<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11 | Loss: 0.1387 | Macro F1: 0.9470 | Micro F1: 0.9477\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/20: 100%|██████████| 335/335 [06:42<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12 | Loss: 0.2063 | Macro F1: 0.9274 | Micro F1: 0.9283\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/20: 100%|██████████| 335/335 [06:45<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13 | Loss: 0.1397 | Macro F1: 0.9365 | Micro F1: 0.9372\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/20: 100%|██████████| 335/335 [06:41<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14 | Loss: 0.1003 | Macro F1: 0.9606 | Micro F1: 0.9611\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/20: 100%|██████████| 335/335 [06:43<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15 | Loss: 0.0595 | Macro F1: 0.9726 | Micro F1: 0.9731\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/20: 100%|██████████| 335/335 [06:44<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16 | Loss: 0.0266 | Macro F1: 0.9939 | Micro F1: 0.9940\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/20: 100%|██████████| 335/335 [06:43<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17 | Loss: 0.0438 | Macro F1: 0.9893 | Micro F1: 0.9895\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/20: 100%|██████████| 335/335 [06:41<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18 | Loss: 0.0492 | Macro F1: 0.9909 | Micro F1: 0.9910\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/20: 100%|██████████| 335/335 [06:37<00:00,  1.19s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19 | Loss: 0.0172 | Macro F1: 0.9954 | Micro F1: 0.9955\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/20: 100%|██████████| 335/335 [06:36<00:00,  1.18s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20 | Loss: 0.0200 | Macro F1: 0.9924 | Micro F1: 0.9925\n","\n","[TEST] BCE Loss: 1.8898\n","[TEST] Macro F1: 0.6642\n","[TEST] Micro F1: 0.6718\n","[TEST] Per-Class F1 Scores:\n"," - Non-violent F1: 0.7147\n"," - Violent F1: 0.6137\n","Confusion Matrix:\n"," [[134  49]\n"," [ 58  85]]\n"]}],"source":["import os, random, numpy as np, pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.transforms import Resize\n","from torch.amp import autocast, GradScaler\n","from torch.utils.checkpoint import checkpoint_sequential\n","from torch.utils.checkpoint import checkpoint\n","\n","# === REPRODUCIBILITY ===\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# === PATHS (COLAB) ===\n","train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/train.csv\"\n","test_csv_path  = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/test.csv\"\n","NPY_DIR        = \"/content/drive/MyDrive/Colab Notebooks/Projects/npy_segments_unimodal\"\n","save_path      = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/CUENET\"\n","os.makedirs(save_path, exist_ok=True)\n","\n","# === CONFIG ===\n","BATCH_SIZE = 2\n","MAX_FRAMES = 80\n","EPOCHS = 20\n","USE_WEIGHTED_LOSS = True\n","PATIENCE = 4\n","\n","# === DATASET ===\n","class ViolenceDataset(Dataset):\n","    def __init__(self, csv_path, npy_dir):\n","        self.df = pd.read_csv(csv_path)\n","        self.npy_dir = npy_dir\n","        self.resize = Resize((224, 224))\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        frames = np.load(os.path.join(self.npy_dir, f\"{row['Segment ID']}.npy\"))[:MAX_FRAMES]\n","        frames = torch.stack([\n","            self.resize(torch.from_numpy(f).permute(2,0,1).float()/255.0)\n","            for f in frames\n","        ])\n","        return frames, torch.tensor(row['Violence label(video)'], dtype=torch.float32)\n","\n","# === CUE-Net Model ===\n","class CUENet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.backbone.fc = nn.Identity()  # remove classifier\n","        self.temporal_gru = nn.GRU(2048, 512, batch_first=True, bidirectional=True)\n","        self.attention_fc = nn.Linear(1024, 1)\n","        self.fc_out = nn.Linear(1024, 1)\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.view(B*T, C, H, W)\n","        # Apply checkpoint to backbone forward\n","        feats = checkpoint(self.backbone, x, use_reentrant=False)\n","        feats = feats.view(B, T, -1)\n","        gru_out, _ = self.temporal_gru(feats)\n","        attn_weights = torch.softmax(self.attention_fc(gru_out), dim=1)\n","        weighted = torch.sum(gru_out * attn_weights, dim=1)\n","        return self.fc_out(weighted).squeeze(1)\n","\n","# === INIT ===\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_dataset = ViolenceDataset(train_csv_path, NPY_DIR)\n","test_dataset  = ViolenceDataset(test_csv_path,  NPY_DIR)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n","\n","pos = train_dataset.df['Violence label(video)'].sum()\n","neg = len(train_dataset) - pos\n","ratio = neg / max(pos, 1)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([ratio]).to(DEVICE)) if USE_WEIGHTED_LOSS else nn.BCEWithLogitsLoss()\n","\n","model = CUENet().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=1, factor=0.5)\n","scaler = GradScaler()\n","\n","best_f1, early_stop_counter = 0, 0\n","\n","# === TRAIN ===\n","for epoch in range(EPOCHS):\n","    model.train()\n","    y_true, y_pred, total_loss = [], [], 0.0\n","    for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n","        frames, labels = frames.to(DEVICE), labels.to(DEVICE)\n","        with autocast(device_type='cuda'):\n","            outputs = model(frames)\n","            loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","        preds = (torch.sigmoid(outputs) > 0.5).int()\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(preds.cpu().numpy())\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","    micro_f1 = f1_score(y_true, y_pred, average='micro')\n","    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Macro F1: {macro_f1:.4f} | Micro F1: {micro_f1:.4f}\")\n","    scheduler.step(macro_f1)\n","    if macro_f1 > best_f1:\n","        best_f1 = macro_f1\n","        torch.save(model.state_dict(), os.path.join(save_path, \"cuenet_best_20.pt\"))\n","        early_stop_counter = 0\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= PATIENCE:\n","            break\n","\n","# === TEST ===\n","model.load_state_dict(torch.load(os.path.join(save_path, \"cuenet_best_20.pt\")))\n","model.eval()\n","y_true, y_pred, test_losses = [], [], []\n","segment_ids = test_dataset.df['Segment ID'].tolist()\n","\n","with torch.no_grad():\n","    for frames, labels in test_loader:\n","        frames, labels = frames.to(DEVICE), labels.to(DEVICE)\n","        outputs = model(frames)\n","        loss = criterion(outputs, labels)\n","        test_losses.append(loss.item())\n","        preds = (torch.sigmoid(outputs) > 0.5).int()\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(preds.cpu().numpy())\n","\n","avg_test_loss = np.mean(test_losses)\n","report = classification_report(y_true, y_pred, target_names=[\"Non-violent\",\"Violent\"], output_dict=True, zero_division=0)\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","print(f\"\\n[TEST] BCE Loss: {avg_test_loss:.4f}\")\n","print(f\"[TEST] Macro F1: {report['macro avg']['f1-score']:.4f}\")\n","print(f\"[TEST] Micro F1: {f1_score(y_true,y_pred,average='micro'):.4f}\")\n","print(\"[TEST] Per-Class F1 Scores:\")\n","print(f\" - Non-violent F1: {report['Non-violent']['f1-score']:.4f}\")\n","print(f\" - Violent F1: {report['Violent']['f1-score']:.4f}\")\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","pd.DataFrame({\"Segment ID\": segment_ids, \"True\": y_true, \"Pred\": y_pred}).to_csv(\n","    os.path.join(save_path, \"cuenet_predictions_20.csv\"), index=False)\n","pd.DataFrame(report).to_csv(os.path.join(save_path, \"cuenet_test_metrics_20.csv\"))\n"]}]}
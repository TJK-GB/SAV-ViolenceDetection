{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1sqaFfNN0_lRJcO-Wjha4wbgGWrtSQ9-_","authorship_tag":"ABX9TyPWo0A8bzWCKSgFyU5o97X8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzzrhFLkYwa1","executionInfo":{"status":"ok","timestamp":1755139234069,"user_tz":-60,"elapsed":4125040,"user":{"displayName":"Taejin Kim","userId":"03444209368555110847"}},"outputId":"bedff932-9a98-4a9e-8062-98d9885d6e82"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/20: 100%|██████████| 335/335 [26:26<00:00,  4.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 0.8173 | Macro F1: 0.5174 | Micro F1: 0.5187\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/20: 100%|██████████| 335/335 [06:23<00:00,  1.14s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 0.7978 | Macro F1: 0.5056 | Micro F1: 0.5097\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/20: 100%|██████████| 335/335 [06:08<00:00,  1.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 0.7918 | Macro F1: 0.4988 | Micro F1: 0.5157\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/20: 100%|██████████| 335/335 [05:59<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 0.7913 | Macro F1: 0.4890 | Micro F1: 0.4918\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/20: 100%|██████████| 335/335 [05:54<00:00,  1.06s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 0.7914 | Macro F1: 0.5052 | Micro F1: 0.5112\n","\n","[TEST] BCE Loss: 0.7912\n","[TEST] Macro F1: 0.3049\n","[TEST] Micro F1: 0.4387\n","[TEST] Per-Class F1 Scores:\n"," - Non-violent F1: 0.0000\n"," - Violent F1: 0.6098\n","Confusion Matrix:\n"," [[  0 183]\n"," [  0 143]]\n"]}],"source":["import os, random, numpy as np, pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import swin_t, Swin_T_Weights\n","from torchvision.transforms import Resize\n","from torch.amp import autocast, GradScaler\n","\n","# === REPRODUCIBILITY ===\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# === PATHS ===\n","train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/train.csv\"\n","test_csv_path  = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/test.csv\"\n","NPY_DIR  = \"/content/drive/MyDrive/Colab Notebooks/Projects/npy_segments_unimodal\"\n","save_path = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/Swin+GRU\"\n","os.makedirs(save_path, exist_ok=True)\n","\n","# === CONFIG ===\n","BATCH_SIZE = 2\n","MAX_FRAMES = 80\n","EPOCHS = 20\n","USE_WEIGHTED_LOSS = True\n","PATIENCE = 4\n","\n","# === MODEL ===\n","class SwinGRUClassifier(nn.Module):\n","    def __init__(self, hidden_size=256, num_layers=1):\n","        super().__init__()\n","        self.swin = swin_t(weights=Swin_T_Weights.DEFAULT)\n","        self.swin.head = nn.Identity()  # Fully unfrozen backbone\n","        self.gru = nn.GRU(\n","            input_size=768,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","        self.fc = nn.Linear(hidden_size * 2, 1)\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.view(B*T, C, H, W)\n","        features = self.swin(x).view(B, T, -1)  # Shape: [B, T, 768]\n","        gru_out, _ = self.gru(features)         # Shape: [B, T, 2*hidden_size]\n","        pooled = gru_out.mean(dim=1)\n","        return self.fc(pooled).squeeze(1)\n","\n","# === DATASET ===\n","class ViolenceDataset(Dataset):\n","    def __init__(self, csv_path, npy_dir):\n","        self.df = pd.read_csv(csv_path)\n","        self.npy_dir = npy_dir\n","        self.resize = Resize((224, 224))\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        frames = np.load(os.path.join(self.npy_dir, f\"{row['Segment ID']}.npy\"))[:MAX_FRAMES]\n","        frames = torch.stack([self.resize(torch.from_numpy(f).permute(2,0,1).float()/255.0) for f in frames])\n","        return frames, torch.tensor(row['Violence label(video)'], dtype=torch.float32)\n","\n","# === INIT ===\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_dataset = ViolenceDataset(train_csv_path, NPY_DIR)\n","test_dataset  = ViolenceDataset(test_csv_path,  NPY_DIR)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n","\n","pos = train_dataset.df['Violence label(video)'].sum()\n","neg = len(train_dataset) - pos\n","ratio = neg / max(pos, 1)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([ratio]).to(device)) if USE_WEIGHTED_LOSS else nn.BCEWithLogitsLoss()\n","\n","model = SwinGRUClassifier().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=1, factor=0.5)\n","scaler = GradScaler()\n","\n","best_f1, early_stop_counter = 0, 0\n","\n","# === TRAIN ===\n","for epoch in range(EPOCHS):\n","    model.train()\n","    y_true, y_pred, total_loss = [], [], 0.0\n","    for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n","        frames, labels = frames.to(device), labels.to(device)\n","        with autocast(device_type='cuda'):\n","            outputs = model(frames)\n","            loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","        preds = (torch.sigmoid(outputs) > 0.5).int()\n","        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","    micro_f1 = f1_score(y_true, y_pred, average='micro')\n","    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Macro F1: {macro_f1:.4f} | Micro F1: {micro_f1:.4f}\")\n","    scheduler.step(macro_f1)\n","    if macro_f1 > best_f1:\n","        best_f1 = macro_f1\n","        torch.save(model.state_dict(), os.path.join(save_path, \"swin_gru_best_20.pt\"))\n","        early_stop_counter = 0\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= PATIENCE: break\n","\n","# === TEST ===\n","model.load_state_dict(torch.load(os.path.join(save_path, \"swin_gru_best_20.pt\")))\n","model.eval()\n","y_true, y_pred, test_losses = [], [], []\n","segment_ids = test_dataset.df['Segment ID'].tolist()\n","with torch.no_grad():\n","    for frames, labels in test_loader:\n","        frames, labels = frames.to(device), labels.to(device)\n","        outputs = model(frames)\n","        loss = criterion(outputs, labels)\n","        test_losses.append(loss.item())\n","        preds = (torch.sigmoid(outputs) > 0.5).int()\n","        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n","\n","avg_test_loss = np.mean(test_losses)\n","report = classification_report(y_true, y_pred, target_names=[\"Non-violent\",\"Violent\"], output_dict=True, zero_division=0)\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","print(f\"\\n[TEST] BCE Loss: {avg_test_loss:.4f}\")\n","print(f\"[TEST] Macro F1: {report['macro avg']['f1-score']:.4f}\")\n","print(f\"[TEST] Micro F1: {f1_score(y_true,y_pred,average='micro'):.4f}\")\n","print(\"[TEST] Per-Class F1 Scores:\")\n","print(f\" - Non-violent F1: {report['Non-violent']['f1-score']:.4f}\")\n","print(f\" - Violent F1: {report['Violent']['f1-score']:.4f}\")\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","pd.DataFrame({\"Segment ID\": segment_ids, \"True\": y_true, \"Pred\": y_pred}).to_csv(\n","    os.path.join(save_path, \"swin_gru_predictions_20epochs.csv\"), index=False)\n","pd.DataFrame(report).to_csv(os.path.join(save_path, \"swin_gru_test_metrics_20epochs.csv\"))\n","# for 10 epochs, change the name by getting rid of _20epochs\n"]}]}
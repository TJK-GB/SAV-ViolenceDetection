{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10876920,
     "status": "ok",
     "timestamp": 1755221933154,
     "user": {
      "displayName": "Taejin Kim",
      "userId": "03444209368555110847"
     },
     "user_tz": -60
    },
    "id": "SDfT39jRKdKt",
    "outputId": "2ddda2c5-b160-456e-dfc3-2d9e1f67601f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:02<00:00, 44.6MB/s]\n",
      "Epoch 1/10: 100%|██████████| 168/168 [29:53<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.8116 | Macro F1: 0.4969 | Micro F1: 0.5052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 168/168 [15:13<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.7922 | Macro F1: 0.5289 | Micro F1: 0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 168/168 [16:06<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.7849 | Macro F1: 0.5438 | Micro F1: 0.5441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 168/168 [14:53<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.7768 | Macro F1: 0.5448 | Micro F1: 0.5486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 168/168 [14:55<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.7674 | Macro F1: 0.5804 | Micro F1: 0.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 168/168 [15:18<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.7707 | Macro F1: 0.5949 | Micro F1: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 168/168 [15:03<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.7678 | Macro F1: 0.5685 | Micro F1: 0.5695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 168/168 [15:22<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.7628 | Macro F1: 0.5964 | Micro F1: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 168/168 [15:15<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.7691 | Macro F1: 0.5725 | Micro F1: 0.5725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 168/168 [16:08<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.7617 | Macro F1: 0.5962 | Micro F1: 0.5964\n",
      "\n",
      "[TEST] BCE Loss: 0.8049\n",
      "[TEST] Macro F1: 0.4942\n",
      "[TEST] Micro F1: 0.5000\n",
      "[TEST] Per-Class F1 Scores:\n",
      " - Non-violent F1: 0.4399\n",
      " - Violent F1: 0.5485\n",
      "Confusion Matrix:\n",
      " [[ 64 119]\n",
      " [ 44  99]]\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "from torchvision.transforms import Resize\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed)\n",
    "torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "#  PATHS change to your directory\n",
    "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/video_source/train.csv\"\n",
    "test_csv_path  = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/video_source/test.csv\"\n",
    "NPY_DIR        = \"/content/drive/MyDrive/Colab Notebooks/npy_segments_videosource\"\n",
    "save_path      = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/Video-source/Swin\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 4\n",
    "MAX_FRAMES = 80\n",
    "EPOCHS = 10\n",
    "USE_WEIGHTED_LOSS = True\n",
    "NUM_SOURCES = 7\n",
    "PATIENCE = 4\n",
    "\n",
    "# Swin model\n",
    "class SwinVideoClassifier_EarlyFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.swin = swin_t(weights=Swin_T_Weights.DEFAULT)\n",
    "        self.swin.head = nn.Identity()  # remove classifier\n",
    "        self.proj = nn.Linear(768 + NUM_SOURCES, 768)  # early fusion with video source one-hot\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, x, src_onehot):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        feats = self.swin(x).view(B, T, -1)  # [B, T, 768]\n",
    "        src_expanded = src_onehot.unsqueeze(1).repeat(1, T, 1)  # [B, T, NUM_SOURCES]\n",
    "        fused = torch.cat([feats, src_expanded], dim=2)\n",
    "        fused = self.proj(fused)\n",
    "        pooled = fused.mean(dim=1)\n",
    "        return self.fc(pooled).squeeze(1)\n",
    "\n",
    "# Dataset load\n",
    "class ViolenceDataset_EarlyFusion(Dataset):\n",
    "    def __init__(self, csv_path, npy_dir):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.npy_dir = npy_dir\n",
    "        self.resize = Resize((224, 224))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        frames_np = np.load(os.path.join(self.npy_dir, f\"{row['Segment ID']}.npy\"))\n",
    "\n",
    "        # convert frames to tensor list\n",
    "        frames_tensors = [self.resize(torch.from_numpy(f).permute(2,0,1).float()/255.0) for f in frames_np]\n",
    "\n",
    "        # pad or truncate\n",
    "        if len(frames_tensors) < MAX_FRAMES:\n",
    "            pad_len = MAX_FRAMES - len(frames_tensors)\n",
    "            frames_tensors.extend([torch.zeros(3,224,224)] * pad_len)\n",
    "        frames_tensors = frames_tensors[:MAX_FRAMES]\n",
    "\n",
    "        frames = torch.stack(frames_tensors)  # [MAX_FRAMES, 3, 224, 224]\n",
    "        label = torch.tensor(row['Violence label(video)'], dtype=torch.float32)\n",
    "\n",
    "        # one-hot encode video source\n",
    "        src_label = int(row['Video Source Label'])\n",
    "        src_onehot = torch.zeros(NUM_SOURCES)\n",
    "        src_onehot[src_label] = 1.0\n",
    "\n",
    "        return frames, label, src_onehot\n",
    "\n",
    "# Train configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = ViolenceDataset_EarlyFusion(train_csv_path, NPY_DIR)\n",
    "test_dataset  = ViolenceDataset_EarlyFusion(test_csv_path,  NPY_DIR)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Weighted loss if needed\n",
    "pos = train_dataset.df['Violence label(video)'].sum()\n",
    "neg = len(train_dataset) - pos\n",
    "ratio = neg / max(pos, 1)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([ratio]).to(device)) if USE_WEIGHTED_LOSS else nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = SwinVideoClassifier_EarlyFusion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=1, factor=0.5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_f1, early_stop_counter = 0, 0\n",
    "\n",
    "# train\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    y_true, y_pred, total_loss = [], [], 0.0\n",
    "    for frames, labels, src_onehot in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        frames, labels, src_onehot = frames.to(device), labels.to(device), src_onehot.to(device)\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(frames, src_onehot)\n",
    "            loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Macro F1: {macro_f1:.4f} | Micro F1: {micro_f1:.4f}\")\n",
    "    scheduler.step(macro_f1)\n",
    "    if macro_f1 > best_f1:\n",
    "        best_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"swin_best.pt\"))\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE: break\n",
    "\n",
    "# test\n",
    "model.load_state_dict(torch.load(os.path.join(save_path, \"swin_best.pt\")))\n",
    "model.eval()\n",
    "y_true, y_pred, test_losses = [], [], []\n",
    "segment_ids = test_dataset.df['Segment ID'].tolist()\n",
    "with torch.no_grad():\n",
    "    for frames, labels, src_onehot in test_loader:\n",
    "        frames, labels, src_onehot = frames.to(device), labels.to(device), src_onehot.to(device)\n",
    "        outputs = model(frames, src_onehot)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Non-violent\",\"Violent\"], output_dict=True, zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"\\n[TEST] BCE Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"[TEST] Macro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"[TEST] Micro F1: {f1_score(y_true,y_pred,average='micro'):.4f}\")\n",
    "print(\"[TEST] Per-Class F1 Scores:\")\n",
    "print(f\" - Non-violent F1: {report['Non-violent']['f1-score']:.4f}\")\n",
    "print(f\" - Violent F1: {report['Violent']['f1-score']:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# save predictions + metrics\n",
    "pd.DataFrame({\"Segment ID\": segment_ids, \"True\": y_true, \"Pred\": y_pred}).to_csv(\n",
    "    os.path.join(save_path, \"swin_predictions.csv\"), index=False)\n",
    "pd.DataFrame(report).to_csv(os.path.join(save_path, \"swin_test_metrics.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOSSdV+UqT5/HRtWJlVlFNZ",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1W3jDUF7cgc5d9ZjwspJLaka3rEybRe5K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

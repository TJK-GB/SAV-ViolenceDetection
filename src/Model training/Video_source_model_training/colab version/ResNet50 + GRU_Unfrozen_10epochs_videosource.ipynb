{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30948164,
     "status": "ok",
     "timestamp": 1755228463044,
     "user": {
      "displayName": "Taejin Kim",
      "userId": "03444209368555110847"
     },
     "user_tz": -60
    },
    "id": "4L8gCotGkmaA",
    "outputId": "6a0c2742-d0e7-469d-e3d9-85f7de7e158a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 168/168 [1:24:48<00:00, 30.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7871 | Macro F1: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 168/168 [49:44<00:00, 17.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.6067 | Macro F1: 0.7406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 168/168 [42:53<00:00, 15.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.3265 | Macro F1: 0.8757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 168/168 [45:46<00:00, 16.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.2303 | Macro F1: 0.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 168/168 [43:47<00:00, 15.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.1795 | Macro F1: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 168/168 [44:22<00:00, 15.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.0748 | Macro F1: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 168/168 [44:44<00:00, 15.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.0876 | Macro F1: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 168/168 [40:04<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.1542 | Macro F1: 0.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 168/168 [38:43<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.0940 | Macro F1: 0.9771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 168/168 [41:27<00:00, 14.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.1392 | Macro F1: 0.9605\n",
      "\n",
      "[TEST] BCE Loss: 1.9962\n",
      "[TEST] Macro F1: 0.4911\n",
      "[TEST] Micro F1: 0.5031\n",
      "Per-Class F1 Scores: 0.5691489361702128 0.41304347826086957\n",
      "Confusion Matrix:\n",
      " [[107  76]\n",
      " [ 86  57]]\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import Resize\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "#  PATHS change to your directory\n",
    "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/video_source/train.csv\"\n",
    "test_csv_path  = \"/content/drive/MyDrive/Colab Notebooks/Projects/CSVs/video_source/test.csv\"\n",
    "NPY_DIR        = \"/content/drive/MyDrive/Colab Notebooks/npy_segments_videosource\"\n",
    "save_path      = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/Video-source/ResNet50\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MAX_FRAMES = 80\n",
    "EPOCHS = 10\n",
    "USE_WEIGHTED_LOSS = True\n",
    "NUM_SOURCES = 7\n",
    "\n",
    "# ResNet50 + GRU Model\n",
    "class ResNet50GRU_EarlyFusion(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Output: 2048-D\n",
    "        self.gru = nn.GRU(2048, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2 + NUM_SOURCES, 1)  # + one-hot source vector\n",
    "\n",
    "    def forward(self, frames, src_onehot):\n",
    "        B, T, C, H, W = frames.shape\n",
    "        x = frames.view(B*T, C, H, W)\n",
    "        feats = self.resnet(x)                     # (B*T, 2048)\n",
    "        feats = feats.view(B, T, -1)               # (B, T, 2048)\n",
    "        _, h_n = self.gru(feats)                   # (num_layers*2, B, hidden_size)\n",
    "        h_n = h_n.permute(1,0,2).reshape(B, -1)    # (B, hidden_size*2)\n",
    "        combined = torch.cat([h_n, src_onehot], dim=1)\n",
    "        return self.fc(combined).squeeze(1)\n",
    "\n",
    "# Dataset load\n",
    "class ViolenceDataset_EarlyFusion(Dataset):\n",
    "    def __init__(self, csv_path, npy_dir):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.npy_dir = npy_dir\n",
    "        self.resize = Resize((224,224))\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        frames_np = np.load(os.path.join(self.npy_dir, f\"{row['Segment ID']}.npy\"))\n",
    "\n",
    "        # Convert to tensor in range [0,1]\n",
    "        frames_tensors = [self.resize(torch.from_numpy(f).permute(2,0,1).float()/255.0) for f in frames_np]\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(frames_tensors) < MAX_FRAMES:\n",
    "            pad_len = MAX_FRAMES - len(frames_tensors)\n",
    "            pad_frames = [torch.zeros(3,224,224)] * pad_len\n",
    "            frames_tensors.extend(pad_frames)\n",
    "        frames_tensors = frames_tensors[:MAX_FRAMES]\n",
    "\n",
    "        frames = torch.stack(frames_tensors)  # Shape: [MAX_FRAMES, 3, 224, 224]\n",
    "\n",
    "        label = torch.tensor(row['Violence label(video)'], dtype=torch.float32)\n",
    "        src_label = int(row['Video Source Label'])\n",
    "        src_onehot = torch.zeros(NUM_SOURCES); src_onehot[src_label] = 1.0\n",
    "        return frames, label, src_onehot\n",
    "\n",
    "# Train configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = ViolenceDataset_EarlyFusion(train_csv_path, NPY_DIR)\n",
    "test_dataset  = ViolenceDataset_EarlyFusion(test_csv_path, NPY_DIR)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "pos = train_dataset.df['Violence label(video)'].sum()\n",
    "neg = len(train_dataset) - pos\n",
    "ratio = neg / max(pos, 1)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([ratio]).to(device)) if USE_WEIGHTED_LOSS else nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = ResNet50GRU_EarlyFusion().to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "# train\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    y_true, y_pred, total_loss = [], [], 0.0\n",
    "    for frames, labels, src_onehot in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        frames, labels, src_onehot = frames.to(device), labels.to(device), src_onehot.to(device)\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(frames, src_onehot)\n",
    "            loss = criterion(outputs, labels)\n",
    "        optimiser.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimiser)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Macro F1: {macro_f1:.4f}\")\n",
    "    if macro_f1 > best_f1:\n",
    "        best_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"resnet50_gru_earlyfusion_best.pt\"))\n",
    "\n",
    "# test\n",
    "model.load_state_dict(torch.load(os.path.join(save_path, \"resnet50_gru_earlyfusion_best.pt\")))\n",
    "model.eval()\n",
    "y_true, y_pred, test_losses = [], [], []\n",
    "segment_ids = test_dataset.df['Segment ID'].tolist()\n",
    "with torch.no_grad():\n",
    "    for frames, labels, src_onehot in test_loader:\n",
    "        frames, labels, src_onehot = frames.to(device), labels.to(device), src_onehot.to(device)\n",
    "        outputs = model(frames, src_onehot)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        y_true.extend(labels.cpu().numpy()); y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Non-violent\",\"Violent\"], output_dict=True, zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "pd.DataFrame({\"Segment ID\": segment_ids, \"True\": y_true, \"Pred\": y_pred}).to_csv(\n",
    "    os.path.join(save_path, \"resnet50_gru_earlyfusion_predictions.csv\"), index=False)\n",
    "pd.DataFrame(report).to_csv(os.path.join(save_path, \"resnet50_gru_earlyfusion_test_metrics.csv\"))\n",
    "\n",
    "print(f\"\\n[TEST] BCE Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"[TEST] Macro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"[TEST] Micro F1: {f1_score(y_true,y_pred,average='micro'):.4f}\")\n",
    "print(\"Per-Class F1 Scores:\", report[\"Non-violent\"][\"f1-score\"], report[\"Violent\"][\"f1-score\"])\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMU8RHPT/xpS5oIees3MP2X",
   "gpuType": "T4",
   "mount_file_id": "1iklXaqp33DJ1sxVP7fIJXjetIwlH7qRY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34532,
     "status": "ok",
     "timestamp": 1755448829775,
     "user": {
      "displayName": "Taejin Kim",
      "userId": "03444209368555110847"
     },
     "user_tz": -60
    },
    "id": "aj0Lo8sNMEOX",
    "outputId": "9f516d46-6e4f-463e-9530-7b66ca803875"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:00<00:00, 250MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attention maps for FN 169_10\n",
      "Saved attention maps for FP 10_1\n",
      "Saved attention maps for TN 102_3\n",
      "Saved attention maps for TP 133_1\n"
     ]
    }
   ],
   "source": [
    "import os, math, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "# Paths change these to your local paths\n",
    "NPY_DIR   = \"/content/drive/MyDrive/Colab Notebooks/Projects/npy_segments_unimodal\"\n",
    "CKPT_PATH = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/Swin+GRU/swin_gru_best.pt\"\n",
    "SAVE_DIR  = \"/content/drive/MyDrive/Colab Notebooks/Results/Interpretability/Again(attention_rollout)/Swin+GRU\"\n",
    "IMG_SIZE  = (224,224)\n",
    "DEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# set segments\n",
    "SEGMENTS = {\n",
    "    \"FN\": (\"169_10\", [0, 20, 40, 60]),\n",
    "    \"FP\": (\"10_1\",   [0, 10, 30, 50]),\n",
    "    \"TN\": (\"102_3\",  [0, 15, 35, 55]),\n",
    "    \"TP\": (\"133_1\",  [0, 25, 45, 65]),\n",
    "}\n",
    "\n",
    "# Swin + GRU\n",
    "class SwinGRUOptional(nn.Module):\n",
    "    def __init__(self, use_gru=True):\n",
    "        super().__init__()\n",
    "        self.use_gru = use_gru\n",
    "        self.swin = swin_t(weights=Swin_T_Weights.IMAGENET1K_V1)\n",
    "        self.swin.head = nn.Identity()  # remove classifier\n",
    "\n",
    "        if self.use_gru:\n",
    "            self.gru = nn.GRU(768, 256, batch_first=True, bidirectional=True)\n",
    "            self.fc  = nn.Linear(512, 1)   # GRU → FC\n",
    "        else:\n",
    "            self.fc  = nn.Linear(768, 1)   # Swin → FC\n",
    "\n",
    "    def forward(self, x):   # GRU: [B,T,C,H,W], Swin: [B,C,H,W]\n",
    "        if self.use_gru:\n",
    "            B,T,C,H,W = x.shape\n",
    "            x = x.view(B*T, C, H, W)\n",
    "            feats = self.swin(x)          # [B*T,768]\n",
    "            feats = feats.view(B, T, -1)  # [B,T,768]\n",
    "            out, _ = self.gru(feats)      # [B,T,512]\n",
    "            out = out[:, -1]              # last timestep\n",
    "            return torch.sigmoid(self.fc(out)), feats\n",
    "        else:\n",
    "            feats = self.swin(x)          # [B,768]\n",
    "            return torch.sigmoid(self.fc(feats)), feats\n",
    "\n",
    "# Attention Rollout\n",
    "class AttentionRollout:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.attentions = []\n",
    "        self.handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        # Only hook into Swin blocks\n",
    "        for stage in self.model.swin.features:\n",
    "            if isinstance(stage, torch.nn.Sequential):\n",
    "                for block in stage:\n",
    "                    if hasattr(block, 'attn'):\n",
    "                        h = block.attn.register_forward_hook(self._get_attention)\n",
    "                        self.handles.append(h)\n",
    "\n",
    "    def _get_attention(self, module, input, output):\n",
    "        qkv = module.qkv(input[0])\n",
    "        if qkv.ndim == 3:\n",
    "            B_, N, _ = qkv.shape\n",
    "            qkv = qkv.reshape(B_, N, 3, module.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        elif qkv.ndim == 4:\n",
    "            B_, H, W, _ = qkv.shape\n",
    "            N = H * W\n",
    "            qkv = qkv.reshape(B_, N, 3, module.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected qkv shape: {qkv.shape}\")\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        head_dim = q.shape[-1]\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale\n",
    "        attn = attn.softmax(dim=-1)  # [B_, heads, N, N]\n",
    "        self.attentions.append(attn.detach().cpu())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.attentions = []\n",
    "        _ = self.model(x)  # forward pass\n",
    "\n",
    "        # take last attention map\n",
    "        attn = self.attentions[-1]   # [B_, heads, N, N]\n",
    "        attn = attn.mean(1)          # avg heads → [B_, N, N]\n",
    "        return attn[0]               # first in batch\n",
    "\n",
    "# Helpers\n",
    "def load_segment(seg_id, frame_indices):\n",
    "    arr = np.load(os.path.join(NPY_DIR, f\"{seg_id}.npy\"))  # [T,H,W,C]\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        f = arr[idx]\n",
    "        if f.ndim == 2:  # grayscale → RGB\n",
    "            f = np.stack([f]*3, axis=-1)\n",
    "        t = torch.from_numpy(f).permute(2,0,1).float()/255.0\n",
    "        t = Resize(IMG_SIZE)(t)\n",
    "        frames.append(t)\n",
    "    return torch.stack(frames)  # [T,C,H,W]\n",
    "\n",
    "def overlay_and_save(img_chw, cam, save_path):\n",
    "    img = img_chw.permute(1,2,0).cpu().numpy().clip(0,1)\n",
    "\n",
    "    # cam: [N, N] (square matrix, e.g. [49,49])\n",
    "    if isinstance(cam, torch.Tensor):\n",
    "        cam = cam.cpu().numpy()\n",
    "\n",
    "    if cam.ndim == 2 and cam.shape[0] == cam.shape[1]:\n",
    "        # collapse query dimension → importance per patch\n",
    "        cam = cam.mean(0)\n",
    "        N = int(cam.shape[0]**0.5) if int(cam.shape[0]**0.5)**2 == cam.shape[0] else cam.shape[0]\n",
    "        if N*N == cam.shape[0]:\n",
    "            cam = cam.reshape(N, N)\n",
    "        else:\n",
    "            cam = cam.reshape(int(math.sqrt(cam.shape[0])), -1)\n",
    "    elif cam.ndim == 1:\n",
    "        N = int(cam.shape[0]**0.5)\n",
    "        cam = cam.reshape(N, N)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected cam shape: {cam.shape}\")\n",
    "\n",
    "    # upscale to full image\n",
    "    cam = torch.tensor(cam).unsqueeze(0).unsqueeze(0)\n",
    "    cam = torch.nn.functional.interpolate(cam, size=IMG_SIZE, mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze().numpy()\n",
    "    cam = (cam - cam.min())/(cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "    heatmap = plt.cm.jet(cam)[...,:3]\n",
    "    vis = (0.5*img + 0.5*heatmap).clip(0,1)\n",
    "    plt.imsave(save_path, vis)\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    USE_GRU = True   # flip this: True = Swin+GRU, False = Swin-only\n",
    "\n",
    "    model = SwinGRUOptional(use_gru=USE_GRU).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    rollout = AttentionRollout(model)\n",
    "\n",
    "    for cat,(seg_id,frame_idx_list) in SEGMENTS.items():\n",
    "        frames = load_segment(seg_id, frame_idx_list).to(DEVICE)  # [T,C,H,W]\n",
    "\n",
    "        if USE_GRU:\n",
    "            frames_in = frames.unsqueeze(0).to(DEVICE)            # [1,T,C,H,W]\n",
    "            cam = rollout(frames_in)\n",
    "            for i,frame in enumerate(frames):\n",
    "                overlay_and_save(frame, cam, os.path.join(SAVE_DIR, f\"{cat}_{seg_id}_f{i}.png\"))\n",
    "        else:\n",
    "            for i,frame in enumerate(frames):\n",
    "                frame_in = frame.unsqueeze(0).to(DEVICE)          # [1,C,H,W]\n",
    "                cam = rollout(frame_in)\n",
    "                overlay_and_save(frame, cam, os.path.join(SAVE_DIR, f\"{cat}_{seg_id}_f{i}.png\"))\n",
    "\n",
    "        print(f\"Saved attention maps for {cat} {seg_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOLqRg3qFToGSoWvU5LYNfX",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "121nVGdUv6HsdnJzj2MLwHXeCqMy-zDul",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

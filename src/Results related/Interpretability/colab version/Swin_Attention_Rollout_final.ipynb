{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22278,
     "status": "ok",
     "timestamp": 1755448686241,
     "user": {
      "displayName": "Taejin Kim",
      "userId": "03444209368555110847"
     },
     "user_tz": -60
    },
    "id": "qBa454e6nfhc",
    "outputId": "b7421f62-f5c5-4967-d041-cc394b18ad1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Attention Rollout maps for FN 169_10\n",
      "Saved Attention Rollout maps for FP 10_1\n",
      "Saved Attention Rollout maps for TN 102_3\n",
      "Saved Attention Rollout maps for TP 119_8\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch, torch.nn as nn, matplotlib.pyplot as plt\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "from torchvision.transforms import Resize\n",
    "import math\n",
    "\n",
    "# Paths change these to your local paths\n",
    "NPY_DIR   = \"/content/drive/MyDrive/Colab Notebooks/Projects/npy_segments_unimodal\"\n",
    "CKPT_PATH = \"/content/drive/MyDrive/Colab Notebooks/Results/Unfrozen_randomseed/Swinonly/swin_best.pt\"\n",
    "SAVE_DIR  = \"/content/drive/MyDrive/Colab Notebooks/Results/Interpretability/Again(attention_rollout)/Swin\"\n",
    "IMG_SIZE  = (224,224)\n",
    "DEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Set segments\n",
    "SEGMENTS = {\n",
    "    \"FN\": (\"169_10\", [0, 20, 40, 60]),\n",
    "    \"FP\": (\"10_1\",   [0, 10, 30, 50]),\n",
    "    \"TN\": (\"102_3\",  [0, 15, 35, 55]),\n",
    "    \"TP\": (\"133_1\",  [0, 25, 45, 65]),\n",
    "}\n",
    "\n",
    "# Swin\n",
    "class SwinWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.swin = swin_t(weights=Swin_T_Weights.IMAGENET1K_V1)\n",
    "        self.swin.head = nn.Identity()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.swin(x)            # [B,768]\n",
    "        return torch.sigmoid(self.fc(feats))\n",
    "\n",
    "# Attention Rollout\n",
    "class AttentionRollout:\n",
    "    def __init__(self, model, discard_ratio=0.0):\n",
    "        self.model = model\n",
    "        self.attentions = []\n",
    "        self.handles = []\n",
    "        self.discard_ratio = discard_ratio\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for stage in self.model.swin.features:   # goes over stages/patch merging/etc.\n",
    "            if isinstance(stage, torch.nn.Sequential):\n",
    "                # this stage contains multiple SwinTransformerBlocks\n",
    "                for block in stage:\n",
    "                    if hasattr(block, 'attn'):   # real transformer block\n",
    "                        h = block.attn.register_forward_hook(self._get_attention)\n",
    "                        self.handles.append(h)\n",
    "            else:\n",
    "                # skip things like PatchMerging\n",
    "                continue\n",
    "\n",
    "    def _get_attention(self, module, input, output):\n",
    "        qkv = module.qkv(input[0])   # [B_, N, 3*dim] or [B_, H, W, 3*dim]\n",
    "\n",
    "        if qkv.ndim == 3:\n",
    "            B_, N, _ = qkv.shape\n",
    "            qkv = qkv.reshape(B_, N, 3, module.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        elif qkv.ndim == 4:\n",
    "            B_, H, W, _ = qkv.shape\n",
    "            N = H * W\n",
    "            qkv = qkv.reshape(B_, N, 3, module.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected qkv shape: {qkv.shape}\")\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        head_dim = q.shape[-1]\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale\n",
    "        attn = attn.softmax(dim=-1)  # [B_, heads, N, N]\n",
    "\n",
    "        self.attentions.append(attn.detach().cpu())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.attentions = []\n",
    "        _ = self.model(x)\n",
    "\n",
    "        # rollout: multiply (I + A) layer by layer\n",
    "        attn = self.attentions[-1]      # [B_, heads, N, N]\n",
    "        attn = attn.mean(1)             # average heads\n",
    "        return attn\n",
    "\n",
    "# Helpers\n",
    "def load_segment(seg_id, frame_indices):\n",
    "    arr = np.load(os.path.join(NPY_DIR, f\"{seg_id}.npy\"))  # [T,H,W,C]\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        f = arr[idx]\n",
    "        if f.ndim == 2:  # grayscale → RGB\n",
    "            f = np.stack([f]*3, axis=-1)\n",
    "        t = torch.from_numpy(f).permute(2,0,1).float()/255.0\n",
    "        t = Resize(IMG_SIZE)(t)\n",
    "        frames.append(t)\n",
    "    return torch.stack(frames)  # [T,C,H,W]\n",
    "\n",
    "def overlay_and_save(img_chw, cam, save_path):\n",
    "    img = img_chw.permute(1,2,0).cpu().numpy().clip(0,1)\n",
    "\n",
    "    # Cam must be numpy, 2D\n",
    "    if isinstance(cam, torch.Tensor):\n",
    "        cam = cam.squeeze()   # remove batch dim → [49,49]\n",
    "        cam = cam.cpu().numpy()\n",
    "\n",
    "    if cam.ndim == 1:\n",
    "        N = int(cam.shape[0] ** 0.5)\n",
    "        cam = cam.reshape(N, N)\n",
    "    elif cam.ndim == 2 and cam.shape[0] == cam.shape[1]:\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected cam shape after squeeze: {cam.shape}\")\n",
    "    # normalise\n",
    "    cam = (cam - cam.min())/(cam.max() - cam.min() + 1e-8)\n",
    "    # upscale to image size\n",
    "    cam = torch.tensor(cam).unsqueeze(0).unsqueeze(0)\n",
    "    cam = torch.nn.functional.interpolate(cam, size=IMG_SIZE, mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze().numpy()\n",
    "\n",
    "    heatmap = plt.cm.jet(cam)[...,:3]\n",
    "    vis = (0.5*img + 0.5*heatmap).clip(0,1)\n",
    "    plt.imsave(save_path, vis)\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    model = SwinWrapper().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    rollout = AttentionRollout(model)\n",
    "\n",
    "    for cat,(seg_id,frame_idx_list) in SEGMENTS.items():\n",
    "        frames = load_segment(seg_id, frame_idx_list).to(DEVICE)  # [T,C,H,W]\n",
    "        for i,frame in enumerate(frames):\n",
    "            frame_in = frame.unsqueeze(0).to(DEVICE)\n",
    "            mask = rollout(frame_in)\n",
    "            overlay_and_save(frame, mask, os.path.join(SAVE_DIR, f\"{cat}_{seg_id}_f{i}.png\"))\n",
    "        print(f\"Saved Attention Rollout maps for {cat} {seg_id}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPoB29oKrZCx8qu5mQJvxTv",
   "mount_file_id": "1x35H3pM783p1wMrs2w-qNNTGkw3S9NtP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
